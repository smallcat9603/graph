{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2faa2a6",
   "metadata": {},
   "source": [
    "# Heterogeneous Node Classification with HashGNN and Autotuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9429a90",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neo4j/graph-data-science-client/blob/main/examples/heterogeneous-node-classification-with-hashgnn.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9030ebb",
   "metadata": {},
   "source": [
    "This Jupyter notebook is hosted [here](https://github.com/neo4j/graph-data-science-client/blob/main/examples/heterogeneous-node-classification-with-hashgnn.ipynb) in the Neo4j Graph Data Science Client Github repository.\n",
    "\n",
    "The notebook exemplifies how to use the `graphdatascience` library to:\n",
    "\n",
    "* Import an IMDB dataset with `Movie`, `Actor` and `Director` nodes directly into GDS using a convenience data loader\n",
    "* Configure a node classification pipeline with HashGNN embeddings for predicting the genre of `Movie` nodes\n",
    "* Train the pipeline with autotuning and inspecting the results\n",
    "* Make predictions for movie nodes missing without a specified genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f661cd4",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Running this notebook requires a Neo4j database server with a recent version (2.3 or newer) of the Neo4j Graph Data Science library (GDS) plugin installed.\n",
    "We recommend using Neo4j Desktop with GDS, or AuraDS.\n",
    "\n",
    "Additionally, the version of the `graphdatascience` library used must be 1.6 or newer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python dependencies\n",
    "%pip install graphdatascience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f4bee",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We start by importing our dependencies and setting up our GDS client connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import os\n",
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Neo4j DB URI, credentials and name from environment if applicable\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_AUTH = None\n",
    "NEO4J_DB = os.environ.get(\"NEO4J_DB\", \"neo4j\")\n",
    "if os.environ.get(\"NEO4J_USER\") and os.environ.get(\"NEO4J_PASSWORD\"):\n",
    "    NEO4J_AUTH = (\n",
    "        os.environ.get(\"NEO4J_USER\"),\n",
    "        os.environ.get(\"NEO4J_PASSWORD\"),\n",
    "    )\n",
    "gds = GraphDataScience(NEO4J_URI, auth=NEO4J_AUTH, database=NEO4J_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec03c6",
   "metadata": {},
   "source": [
    "## Loading the IMDB dataset\n",
    "\n",
    "Next we use the `graphdatascience` [built-in IMDB loader](https://neo4j.com/docs/graph-data-science-client/current/common-datasets/#_imdb) to get data into our GDS server.\n",
    "This should give us a graph with `Movie`, `Actor` and `Director` nodes, connected by `ACTED_IN` and `DIRECTED_IN` relationships.\n",
    "\n",
    "Note that a \"real world scenario\", we would probably project our own data from a Neo4j database into GDS instead, or using `gds.alpha.graph.construct` to create a graph from our own client side data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156843c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the loading to obtain a `Graph` object representing our GDS projection\n",
    "G = gds.graph.load_imdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95eb00b",
   "metadata": {},
   "source": [
    "Let's inspect our graph to see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478eaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Overview of G: {G}\")\n",
    "print(f\"Node labels in G: {G.node_labels()}\")\n",
    "print(f\"Relationship types in G: {G.relationship_types()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87741f34",
   "metadata": {},
   "source": [
    "It looks as expected, though we notice that some nodes are of a `UnclassifiedMovie` label.\n",
    "Indeed, these are the nodes whose genre we wish to predict with a node classification model.\n",
    "Let's look at the node properties present on the various node labels to see this more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c5def",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node properties per label:\\n{G.node_properties()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3fd5e0",
   "metadata": {},
   "source": [
    "So we see that the `Movie` nodes have the `genre` property, which means that we can use these nodes when training our model later.\n",
    "The `UnclassifiedMovie` nodes as expected does not have the `genre` property, which is exactly what we want to predict.\n",
    "\n",
    "Additionally, we notice that all nodes have a `plot_keywords` property.\n",
    "This is a binary \"bag-of-words\" type feature vector representing which out of 1256 plot keywords describe a certain node.\n",
    "These feature vectors will be used as input to our HashGNN node embedding algorithm later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f08b5e",
   "metadata": {},
   "source": [
    "## Configuring the training pipeline\n",
    "\n",
    "Now that we loaded and understood the data we want to analyze, we can move on to look at the tools for actually making the aforementioned genre predictions of the `UnclassifiedMovie` nodes.\n",
    "\n",
    "Since we want to predict discrete valued properties of nodes, we will use a [node classification pipeline](https://neo4j.com/docs/graph-data-science-client/current/pipelines/#_node_classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d11361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty node classification pipeline\n",
    "pipe, _ = gds.beta.pipeline.nodeClassification.create(\"genre-predictor-pipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cba958",
   "metadata": {},
   "source": [
    "To be able to compare our accuracy score to the current state of the art methods on this dataset, we want to use the same test set size as in the Graph Transformer Network paper ([NIPS paper link](https://proceedings.neurips.cc/paper/2019/file/9d63484abb477c97640154d40595a3bb-Paper.pdf)).\n",
    "We configure our pipeline accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aacdd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test set size to be 79.6 % of the entire set of `Movie` nodes\n",
    "_ = pipe.configureSplit(testFraction=0.796)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20449f74",
   "metadata": {},
   "source": [
    "Please note that we would get a much better model by using a more standard train-test split, like 80/20 or so.\n",
    "And typically this would be the way to go for real use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0dbcc",
   "metadata": {},
   "source": [
    "### The HashGNN node embedding algorithm\n",
    "\n",
    "As the last part of the training pipeline, there will be an ML training algorithm.\n",
    "If we use the `plot_keywords` directly as our feature input to the ML algorithm, we will not utilize any of the relationship data we have in our graph.\n",
    "Since relationships would likely enrich our features with more valuable information, we will use a node embedding algorithm which takes relationships into account, and use its output as input to the ML training algorithm.\n",
    "\n",
    "In this case we will use the HashGNN node embedding algorithm which is new in GDS 2.3.\n",
    "Contrary to what the name suggests, HashGNN is not a supervised neural learning model.\n",
    "It is in fact an unsupervised algorithm.\n",
    "Its name comes from the fact that the algorithm design is inspired by that of graph neural networks, in that it does message passing interleaved with transformations on each node.\n",
    "But instead of doing neural transformations like most GNNs, its transformations are done by locality sensitive min-hashing.\n",
    "Since the hash functions used are randomly chosen independent of the input data, there is no need for training. \n",
    "\n",
    "We will give hashGNN the `plot_keywords` node properties as input, and it will output new feature vectors for each node that has been enriched by message passing over relationships.\n",
    "Since the `plot_keywords` vectors are already binary we don't have to do any [binarization](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/#_feature_binarization_2) of the input.\n",
    "\n",
    "Since we have multiple node labels and relationships, we make sure to enable the heterogeneous capabilities of HashGNN by setting `heterogeneous=True`.\n",
    "Notably we also declare that we want to include all kinds of nodes, not only the `Movie` nodes we will train on, by explicitly specifying the `contextNodeLabels`.\n",
    "\n",
    "Please see the [HashGNN documentation](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/) for more on this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e254efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a HashGNN node property step to the pipeline\n",
    "_ = pipe.addNodeProperty(\n",
    "    \"beta.hashgnn\",\n",
    "    mutateProperty=\"embedding\",\n",
    "    iterations=4,\n",
    "    heterogeneous=True,\n",
    "    embeddingDensity=512,\n",
    "    neighborInfluence=0.7,\n",
    "    featureProperties=[\"plot_keywords\"],\n",
    "    randomSeed=41,\n",
    "    contextNodeLabels=G.node_labels(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the embeddings vectors produced by HashGNN as feature input to our ML algorithm\n",
    "_ = pipe.selectFeatures(\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f2bc1",
   "metadata": {},
   "source": [
    "## Setting up autotuning\n",
    "\n",
    "It is time to set up the [ML algorithms](https://neo4j.com/docs/graph-data-science/current/machine-learning/training-methods/) for the training part of the pipeline.\n",
    "\n",
    "In this example we will add logistic regression and random forest algorithms as candidates for the final model.\n",
    "Each candidate will be evaluated by the pipeline, and the best one, according to our specified metric, will be chosen.\n",
    "\n",
    "It is hard to know how much regularization we need so as not to overfit our models on the training dataset, and for this reason we will use the autotuning capabilities of GDS to help us out.\n",
    "The autotuning algorithm will try out several values for the regularization parameters `penalty` (of logistic regression) and `minSplitSize` (of random forest) and choose the best ones it finds.\n",
    "\n",
    "Please see the GDS manual to learn more about [autotuning](https://neo4j.com/docs/graph-data-science/current/machine-learning/auto-tuning/), [logistic regression](https://neo4j.com/docs/graph-data-science/current/machine-learning/training-methods/logistic-regression/) and [random forest](https://neo4j.com/docs/graph-data-science/current/machine-learning/training-methods/random-forest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccd326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add logistic regression as a candidate ML algorithm for the training\n",
    "# Provide an interval for the `penalty` parameter to enable autotuning for it\n",
    "_ = pipe.addLogisticRegression(penalty=(0.1, 1.0), maxEpochs=1000, patience=5, tolerance=0.0001, learningRate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fca4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random forest as a candidate ML algorithm for the training\n",
    "# Provide an interval for the `minSplitSize` parameter to enable autotuning for it\n",
    "_ = pipe.addRandomForest(minSplitSize=(2, 100), criterion=\"ENTROPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23601d62",
   "metadata": {},
   "source": [
    "## Training the pipeline\n",
    "\n",
    "The configuration is done, and we are now ready to kick off the training of our pipeline and see what results we get.\n",
    "\n",
    "In our training call, we provide what node label and property we want the training to target, as well as the metric that will determine how the best model candidate is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call train on our pipeline object to run the entire training pipeline and produce a model\n",
    "model, _ = pipe.train(\n",
    "    G,\n",
    "    modelName=\"genre-predictor-model\",\n",
    "    targetNodeLabels=[\"Movie\"],\n",
    "    targetProperty=\"genre\",\n",
    "    metrics=[\"F1_MACRO\"],\n",
    "    randomSeed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cba672",
   "metadata": {},
   "source": [
    "Let's inspect the model that was created by the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy scores of trained model:\\n{model.metrics()['F1_MACRO']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df73b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Winning ML algorithm candidate config:\\n{model.best_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b370419",
   "metadata": {},
   "source": [
    "As we can see the best ML algorithm configuration that the autotuning found was logistic regression with `penalty=0.159748`.\n",
    "\n",
    "Further we note that the test set F1 score is 0.59118347, which is really good to when comparing to scores of other algorithms on this dataset in the literature.\n",
    "More on this in the [Conclusion](#Conclusion) section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ace2bd",
   "metadata": {},
   "source": [
    "## Making new predictions\n",
    "\n",
    "We can now use the model produced by our training pipeline to predict genres of the `UnclassifiedMovie` nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4419924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict `genre` for `UnclassifiedMovie` nodes and stream the results\n",
    "predictions = model.predict_stream(G, targetNodeLabels=[\"UnclassifiedMovie\"], includePredictedProbabilities=True)\n",
    "\n",
    "print(f\"First predictions of unclassified movie nodes:\\n{predictions.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af336c0",
   "metadata": {},
   "source": [
    "In this case we streamed the prediction results back to our client application, but we could for example also have mutated our GDS graph represented by `G` by calling `model.predict_mutate` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9c5d5",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "Optionally we can now clean up our GDS state, to free up memory for other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the GDS graph represented by `G` from the GDS graph catalog\n",
    "_ = G.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b96625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the GDS training pipeline represented by `pipe` from the GDS pipeline catalog\n",
    "_ = pipe.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the GDS model represented by `model` from the GDS model catalog\n",
    "_ = model.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda459c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By using only the GDS library and its client, we were able to train a node classification model using the sophisticated HashGNN node embedding algorithm and logistic regression.\n",
    "Our logistic regression configuration was automatically chosen as the best candidate among a number of other algorithms (like random forest with various configurations) through a process of autotuning.\n",
    "We were able to achieve this with very little code, and with very good scores.\n",
    "\n",
    "Though we used a convenience method of the `graphdatascience` library to load an IMDB dataset into GDS, it would be very easy to replace this part with something like a [projection from a Neo4j database](https://neo4j.com/docs/graph-data-science-client/current/graph-object/#_projecting_a_graph_object) to create a more realistic production workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c38072",
   "metadata": {},
   "source": [
    "### Comparison with other methods\n",
    "\n",
    "As mentioned we tried to mimic the setup of the benchmarks in the NeurIPS paper [Graph Transformer Networks](https://proceedings.neurips.cc/paper/2019/file/9d63484abb477c97640154d40595a3bb-Paper.pdf), in order to compare with the current state of the art methods.\n",
    "A difference from this paper is that they have a predefined train-test set split, whereas we just generate a split (with the same size) uniformly at random within our training pipeline. However, we have no reason to think that the predefined split in the paper was not also generated uniformly at random.\n",
    "Additionally, they use length 64 float embeddings (64 * 32 = 2048 bits), whereas we use length 1256 bit embeddings with HashGNN.\n",
    "\n",
    "The scores they observe are the following:\n",
    "\n",
    "| Algorithm    | Test set F1 score (%) |\n",
    "| ------------ | --------------------- |\n",
    "| DeepWalk     | 32.08                 |\n",
    "| metapath2vec | 35.21                 |\n",
    "| GCN          | 56.89                 |\n",
    "| GAT          | 58.14                 |\n",
    "| HAN          | 56.77                 |\n",
    "| GTN          | 60.92                 |\n",
    "\n",
    "In light of this, it is indeed very impressive that we get a test set F1 score of 59.11 % with HashGNN and logistic regression.\n",
    "Especially considering that:\n",
    "- we use fewer bits to represent the embeddings (1256 vs 2048)\n",
    "- use dramatically fewer training parameters in our gradient descent compared to the deep learning models above\n",
    "- HashGNN is an unsupervised algorithm\n",
    "- HashGNN runs a lot faster (even without a GPU) and requires a lot less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555ab36",
   "metadata": {},
   "source": [
    "### Further learning\n",
    "\n",
    "To learn more about the topics covered in this notebook, please check out the following pages of the GDS manual:\n",
    "\n",
    "- [Node Classification Pipelines](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-property-prediction/nodeclassification-pipelines/node-classification/)\n",
    "- [HashGNN](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)\n",
    "- [Logistic Regression](https://neo4j.com/docs/graph-data-science/current/machine-learning/training-methods/logistic-regression/)\n",
    "- [Random Forest](https://neo4j.com/docs/graph-data-science/current/machine-learning/training-methods/random-forest/)\n",
    "- [Autotuning](https://neo4j.com/docs/graph-data-science/current/machine-learning/auto-tuning/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
